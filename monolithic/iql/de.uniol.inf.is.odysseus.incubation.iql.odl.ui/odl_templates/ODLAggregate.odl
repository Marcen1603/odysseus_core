operator ODLAggregate(outputMode = "NEW_ELEMENT", minInputPorts = 1, maxInputPorts = 1) {
		
	optional parameter(type=ResolvedSDFAttributeParameter) SDFAttribute[] group_by;
	
	parameter AggregateItem[] aggregations;
			
	List outputAttributList = [];		
	Map aggregationsMap = new HashMap();
	Map outAttributeToAggregation = new HashMap();
	List groupingAttributes = new ArrayList();	
	IMetadataMergeFunction metadataMerge;
	IGroupProcessor groupProcessor;	
	ITransferArea transferArea  = new TITransferArea();
	Map groups = new HashMap();
	long createOutputCounter = 0;
	Map init = new HashMap();
	Map merger = new HashMap();
	Map eval = new HashMap();
		
	on ao_init {
		for (AggregateItem item : aggregations) {
			addAggregation(item.inAttributes, item.aggregateFunction,item.outAttribute);
		}		
		if (group_by != null) {
			for (SDFAttribute attr : group_by) {
				addGroupingAttribute(attr);
			}	
		}	
	}
	
	on createOutputSchema(int port) : SDFSchema{							
		SDFAttribute[] outAttribs = new ArrayList();

		for (Object obj : outputAttributList) {
			SDFAttribute e1 = ((IPair)obj).e1;
			outAttribs += e1;
		}
		
		if (getInputSchema(0) != null) {
			return SDFSchemaFactory::createNewWithAttributes(outAttribs, getInputSchema(0));
		} else {
			return SDFSchemaFactory::createNewSchema("<tmp>", class(Tuple), outAttribs);		
		}		
	}
	
	on processOpen {
		if (init.isEmpty || merger.isEmpty || eval.isEmpty) {
			for (Object obj : aggregationsMap.keySet) {
				SDFSchema attrList = obj;
				if (SDFSchema::subset(attrList, getInputSchema(0))) {
					Map funcs = aggregationsMap.get(attrList);
					for (Object o : funcs.entrySet) {
						Entry e = o;
						FESortedClonablePair p = new FESortedClonablePair(attrList, e.key);
						int[] posArray = new int[];
						boolean partialAggregateInput = false;
						String datatype = null;
						SDFSchema schema = p.e1;
						for (int i = 0; i < schema.size; ++i) {
							SDFAttribute attr = schema.get(i);
							posArray[i] = getInputSchema(0).indexOf(attr);
							if (attr.datatype.isPartialAggregate) {
								partialAggregateInput = true;
							}
							datatype = attr.datatype.uRI;
						}		
						AggregateFunction function = p.e2;	
						IAggregateFunctionBuilder builder = AggregateFunctionBuilderRegistry::getBuilder(getInputSchema(0).type, function.name);
				
						IAggregateFunction aggFunction = builder.createAggFunction(function, schema, posArray, partialAggregateInput,datatype);
						init.put(p, aggFunction);
						merger.put(p, aggFunction);
						eval.put(p, aggFunction);
					}
				}
			}
		}
		
		if (groupProcessor == null) {
			groupProcessor = new RelationalGroupProcessor(getInputSchema(0), this.outputSchema, groupingAttributes, aggregationsMap, false);
		}
		if (metadataMerge == null) {
			List metadataSet = getInputSchema(0).metaAttributeNames;
			metadataSet.remove(class(ITimeInterval).name);
			metadataMerge = MetadataRegistry::getMergeFunction(metadataSet);
		}
		
		groupProcessor.init;
		transferArea.init(this, this.subscribedToSource.size);
		groups.clear;
		createOutputCounter = 0;
	}
	
	
	on processNext(Tuple tuple, int port) {
		
		Long groupID = groupProcessor.getGroupID(tuple);
		AggregateTISweepArea sa = groups.get(groupID);
		if (sa == null) {
			sa = new AggregateTISweepArea();
			groups.put(groupID, sa);
		}
		List results = updateSA(sa, tuple, false);
		ITimeInterval time = tuple.metadata;
		createOutput(results, groupID, time.start, port, groups, groupProcessor);
	}
	
	on processPunctuation(IPunctuation punctuation, int port) {
		transferArea.sendPunctuation(punctuation, port);
		createOutput(null, null, punctuation.time, port, groups, groupProcessor);		
	}
	on processDone {
		transferArea.done(0);
	}
	
	po createOutput(List existingResults, Long groupID,	PointInTime timestamp, int inPort,	Map groupsToProcess, IGroupProcessor g) {

		PointInTime border = timestamp;

		if (existingResults != null) {
			produceResults(existingResults, groupID, g);
		}


		for (Object obj : groupsToProcess.entrySet) {
			Entry entry = obj;
			AggregateTISweepArea sa = entry.value;

			Iterator results = sa.extractElementsBefore(timestamp);
				
			produceResults(results, entry.key, g);
			PointInTime sa_min_ts = sa.calcMinTs;

			if (sa_min_ts != null) {
				if (sa_min_ts.before(border)) {
					border = sa_min_ts;
				}
			}
		}		
		transferArea.newHeartbeat(border, inPort);
	}
	
	po produceResults(List results, Long groupID, IGroupProcessor g) {
		for (Object obj : results) {
			PairMap e = obj;
			Tuple out = g.createOutputElement(groupID, e);
			ITimeInterval newMeta = e.metadata.clone;
			out.metadata = newMeta;
			transferArea.transfer(out);
		}
	}
	
	po  produceResults(Iterator results, Long groupID, IGroupProcessor g) {
		while (results.hasNext) {
			PairMap e = results.next;
			Tuple out = null;
			PairMap r = calcEval(e, true);
			out = g.createOutputElement(groupID, r);
			ITimeInterval newMeta = e.metadata.clone;
			out.metadata = newMeta;
			transferArea.transfer(out);
		}
	}
	
	po calcEval(PairMap toEval, boolean clearPartialAggregate) : PairMap{
		PairMap ret = new PairMap();
		for (Object obj: toEval.entrySet) {
			Entry e = obj;
			IEvaluator eva = eval.get(e.key);
			Tuple value = eva.evaluate(e.value);
			ret.put(e.key, value);
			if (clearPartialAggregate){
				IPartialAggregate agg = e.value;
				agg.clear;
			}
		}
		return ret;
	}
	
	po updateSA(AggregateTISweepArea sa, Tuple elemToAdd, boolean outputPA) : List {
		List returnValues = new LinkedList();
		ITimeInterval t_probe = elemToAdd.metadata;
		Iterator qualifies = sa.extractOverlaps(t_probe);
		
		if (!qualifies.hasNext) {
			saInsert(sa, calcInit(elemToAdd), t_probe);
		} else {
			List pl = getSortedIntersectionPoints(t_probe, qualifies);

			Iterator pointIter = pl.iterator;
			
			_Point p1 = null;
			_Point p2 = null;


			p1 = pointIter.next;
			PairMap lastPartialAggregate = p1.load;
			
			while (pointIter.hasNext) {
				p2 = pointIter.next;
				if (p1.before(p2)) {

					if (p1.start && p2.start) {
						lastPartialAggregate = updateSAStartStart(sa, elemToAdd, p1, p2, lastPartialAggregate);

					} else if (p1.start && p2.end) {

						updateSAStartEnd(sa, elemToAdd, outputPA, returnValues, t_probe, p1, p2, lastPartialAggregate);

					} else if (p1.end && p2.start) {
						updateSAEndStart(sa, elemToAdd, p1, p2);
					} else if (p1.end && p2.end) {
						lastPartialAggregate = updateSAEndEnd(sa, elemToAdd, p1, p2, lastPartialAggregate);
					}
					if (p1.load != null) {
						lastPartialAggregate = p1.load;						
					} 
				} else {
					if (p2.load != null) {
						lastPartialAggregate = p2.load;						
					} 
				}

				p1 = p2;
			}
		}
		return returnValues;
	}	
	
	po updateSAStartEnd(AggregateTISweepArea sa, Tuple elemToAdd, boolean outputPA, List returnValues, ITimeInterval t_probe,_Point p1,_Point p2, PairMap lastPartialAggregate) {
		boolean createNew = false;
		if (!outputPA && p2.before(t_probe.start)) {
			createNew = false;
			PairMap v = calcEval(lastPartialAggregate, false);
			ITimeInterval meta = lastPartialAggregate.metadata.clone;
			v.metadata = meta;
			meta.end = p2.point;
			returnValues.add(v);
		} else {
			ITimeInterval meta = lastPartialAggregate.metadata;
			createNew = !(p1.point.equals(meta.start)
					&& p2.point.equals(meta.end));
		}

		ITimeInterval newMeta = metadataMerge.mergeMetadata(lastPartialAggregate.metadata, elemToAdd.metadata);
		newMeta = newMeta.clone;
		newMeta.setStartAndEnd(p1.point, p2.point);
		saInsert(sa, calcMerge(lastPartialAggregate, elemToAdd, createNew), newMeta);
	}
	
	po calcMerge(PairMap toMerge, Tuple element, boolean createNew) : PairMap {		
		PairMap ret = new PairMap();
		for (Object obj : toMerge.entrySet) {
			Entry e = obj;
			IMerger mf = merger.get(e.key);
			IPartialAggregate pa = mf.merge(e.value, element, createNew);
			ret.put(e.key, pa);
		}

		return ret;
	}

	
	po getSortedIntersectionPoints(ITimeInterval t_probe, Iterator qualifies) : List {		
		List pl = new LinkedList();

		while (qualifies.hasNext) {
			PairMap element_agg = qualifies.next;
			ITimeInterval t_agg = element_agg.metadata;
			pl.add(new _Point(t_agg.start, true,element_agg));
			pl.add(new _Point(t_agg.end, false, element_agg));
		}
		pl.add(new _Point(t_probe.start, true,	null));
		pl.add(new _Point(t_probe.end, false,	null));

		Collections::sort(pl);
		return pl;
	}

	po updateSAStartStart(AggregateTISweepArea sa, Tuple elemToAdd,	_Point p1, _Point p2, PairMap lastPartialAggr) : PairMap{
		PairMap lastPartialAggregate = lastPartialAggr;
		if (p1.load == null) {			
			ITimeInterval meta = elemToAdd.metadata;
			ITimeInterval newMeta = meta.clone;
			newMeta.setStartAndEnd(p1.point, p2.point);
			saInsert(sa, calcInit(elemToAdd), newMeta);
			lastPartialAggregate = p2.load;
		} else {
			PairMap pairMap = p1.load;	
			ITimeInterval meta = pairMap.metadata;
			ITimeInterval newMeta = meta.clone;
			newMeta.setStartAndEnd(p1.point, p2.point);
			saInsert(sa, pairMap, newMeta);
		}
		return lastPartialAggregate;
	}
	
	po updateSAEndEnd(AggregateTISweepArea sa, Tuple elemToAdd,	_Point p1,	_Point p2,	PairMap partialAggregate) : PairMap {
		if (p2.load == null) {
			updateSAEndStart(sa, elemToAdd, p1, p2);
		} else {
			PairMap newPA = partialAggregate.clone;
			ITimeInterval newTI = partialAggregate.metadata.clone;
			newTI.setStartAndEnd(p1.point, p2.point);
			saInsert(sa, newPA, newTI);
			partialAggregate = newPA;
		}
		return partialAggregate;
	}
	
	po updateSAEndStart(AggregateTISweepArea sa, Tuple elemToAdd,_Point p1,_Point p2) {
		ITimeInterval newTI = elemToAdd.metadata.clone;
		newTI.setStartAndEnd(p1.point, p2.point);
		saInsert(sa, calcInit(elemToAdd), newTI);
	}
	
	
	po calcInit(Tuple element) : PairMap {
		PairMap ret = new PairMap();
		for (Object o : aggregationsMap.keySet) {
			SDFSchema attrList = o;
			if (SDFSchema::subset(attrList, getInputSchema(0))) {
				Map funcs = aggregationsMap.get(attrList);
				for (Object obj : funcs.entrySet) {		
					Entry e = obj;			
					FESortedClonablePair toFind = new FESortedClonablePair(attrList, e.key);
					IInitializer initFktn = init.get(toFind);					
					IPartialAggregate pa = initFktn.init(element);
					ret.put(attrList, e.key, pa);
				}
			}
		}
		return ret;
	}
	
	po saInsert(AggregateTISweepArea sa, PairMap elem, ITimeInterval t) {
		elem.metadata = t;
		sa.insert(elem);
	}	
	
	ao addGroupingAttributes(SDFSchema attributes) {
		for (SDFAttribute a : attributes) {
			addGroupingAttribute(a);
		}
	}
	
	ao addGroupingAttribute(SDFAttribute attribute) {
		if (groupingAttributes.contains(attribute)) {
			return;
		}
		groupingAttributes.add(attribute);
		outputAttributList.add(new Pair(attribute, false));
		setOutputSchema(null);
	}
	
	ao addAggregation(SDFAttribute[] attributes, AggregateFunction function, SDFAttribute outAttribute) {
		SDFSchema schema = SDFSchemaFactory::createNewSchema("", class(Tuple), attributes);
		addAggregation(schema, function, outAttribute);
	}

	ao addAggregation(SDFSchema attributes,	AggregateFunction function, SDFAttribute outAttribute) {	
		outputAttributList.add(new Pair(outAttribute, true));
		setOutputSchema(null);
		Map af = aggregationsMap.get(attributes);
		if (af == null) {
			af = new HashMap();
			aggregationsMap.put(attributes, af);
		}
		af.put(function, outAttribute);
		outAttributeToAggregation.put(outAttribute, function);
	}
}


class _Point implements Comparable {
	PointInTime point;

	boolean isStartPoint;

	Object load;

	_Point(PointInTime p, boolean isStartPoint, Object element_agg) {
		this.point = p;
		this.isStartPoint = isStartPoint;
		this.load = element_agg;
	}


	override compareTo(Object obj) : int {
		_Point p2 = obj;
		int c = point.compareTo(p2.point);
		if (c == 0) {
			if (isStartPoint && !p2.isStartPoint) {
				c = 1;
			} else if (!isStartPoint && p2.isStartPoint) {
				c = -1;
			}
		}
		if (c == 0) {
			if (newElement) {
				if (isStartPoint) {
					c = 1;
				} else {
					c = -1;
				}
			}
		}
		return c;
	}

	isStart : boolean {
		return isStartPoint;
	}

	isEnd : boolean {
		return !isStartPoint;
	}

	getLoad : Object {
		return load;
	}

	getPoint : PointInTime{
		return point;
	}

	before(_Point other) : boolean{
		return point.before(other.point);
	}

	before(PointInTime other) : boolean{
		return point.before(other);
	}

	override hashCode : int {
		int PRIME = 31;
		int result = 1;
		if (point == null) {
			result = PRIME * result + 0;			
		} else {
			result = PRIME * result + point.hashCode;			
		}
		if (isStartPoint) {
			result = PRIME * result + 1231;			
		} else {
			result = PRIME * result + 1237;			
		}
		return result;
	}

	override equals(Object obj) : boolean {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (this.getClass != obj.getClass)
			return false;
		_Point other =  obj;
		if (point == null) {
			if (other.point != null)
				return false;
		} else if (!point.equals(other.point))
			return false;
		if (isStartPoint != other.isStartPoint)
			return false;
		return true;
	}

	newElement : boolean {
		return load == null;
	}

	override toString : String {
		String result = "";
		if (isStartPoint) {
			result = "s";
		} else {
			result =  "e";
		}
		if (newElement) {
			result = result +"^";
		} 
		return result + point;
	}
}


