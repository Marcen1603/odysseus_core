operator ODLAggregate(outputMode = "NEW_ELEMENT", minInputPorts = 1, maxInputPorts = 1) {
		
	optional parameter(type=ResolvedSDFAttributeParameter) SDFAttribute[] group_by;
	
	parameter AggregateItem[] aggregations;
			
	List outputAttributList = [];
		
	Map aggregationsMap = new HashMap();
	Map outAttributeToAggregation = new HashMap();
	List groupingAttributes = new ArrayList();
	
	
	IMetadataMergeFunction metadataMerge;
	IGroupProcessor groupProcessor;	
	ITransferArea transferArea  = new TITransferArea();
	
	Map groups = new HashMap();
	long createOutputCounter = 0;
	Map init = new HashMap();
	Map merger = new HashMap();
	Map eval = new HashMap();
	
	
	on ao_init() {
		for (AggregateItem item : aggregations) {
			addAggregation(item.inAttributes, item.aggregateFunction,item.outAttribute);
		}
		if (group_by != null) {
			for (SDFAttribute attr : group_by) {
				addGroupingAttribute(attr);
			}	
		}	
	}
	
	on processOpen() {
		if (init.isEmpty || merger.isEmpty || eval.isEmpty) {
			for (Object obj : aggregationsMap.keySet()) {
				SDFSchema attrList = obj;
				if (SDFSchema::subset(attrList, getInputSchema(0))) {
					Map funcs = aggregationsMap.get(attrList);
					for (Object o : funcs.entrySet()) {
						Entry e = o;
						FESortedClonablePair p = new FESortedClonablePair(attrList, e.getKey());
						int[] posArray = new int[];
						boolean partialAggregateInput = false;
						String datatype = null;
						SDFSchema schema = p.getE1();
						for (int i = 0; i < schema.size(); ++i) {
							SDFAttribute attr = schema.get(i);
							posArray[i] = getInputSchema(0).indexOf(attr);
							if (attr.getDatatype().isPartialAggregate()) {
								partialAggregateInput = true;
							}
							datatype = attr.getDatatype().getURI();
						}		
						AggregateFunction function = p.getE2();	
						IAggregateFunctionBuilder builder = AggregateFunctionBuilderRegistry::getBuilder(getInputSchema(0).getType(), function.getName());
				
						IAggregateFunction aggFunction = builder.createAggFunction(function, schema, posArray, partialAggregateInput,datatype);
						init.put(p, aggFunction);
						merger.put(p, aggFunction);
						eval.put(p, aggFunction);
					}
				}
			}
		}
		
		if (groupProcessor == null) {
			groupProcessor = new RelationalGroupProcessor(getInputSchema(0), getOutputSchema(0), groupingAttributes, aggregationsMap, false);
		}
		if (metadataMerge == null) {
			List metadataSet = getInputSchema(0).getMetaAttributeNames();
			metadataSet.remove(class(ITimeInterval).getName());
			metadataMerge = MetadataRegistry::getMergeFunction(metadataSet);
		}
		
		groupProcessor.init();
		transferArea.init(this, getSubscribedToSource().size());
		groups.clear();
		createOutputCounter = 0;
	}
	
	on processDone() {
		transferArea.done(0);
	}
	
	on processNext(Tuple tuple, int port) {
		
		Long groupID = groupProcessor.getGroupID(tuple);
		AggregateTISweepArea sa = groups.get(groupID);
		if (sa == null) {
			sa = new AggregateTISweepArea();
			groups.put(groupID, sa);
		}
		List results = updateSA(sa, tuple, false);
		ITimeInterval time = tuple.getMetadata;
		createOutput(results, groupID, time.getStart(), port, groups, groupProcessor);
	}
	
	on processPunctuation(IPunctuation punctuation, int port) {
		transferArea.sendPunctuation(punctuation, port);
		createOutput(null, null, punctuation.getTime(), port, groups, groupProcessor);		
	}
			
	on createOutputSchema(int port) : SDFSchema{							
		SDFAttribute[] outAttribs = new ArrayList();

		for (Object obj : outputAttributList) {
			SDFAttribute e1 = ((IPair)obj).getE1();
			outAttribs += e1;
		}
		
		if (getInputSchema(0) != null) {
			return SDFSchemaFactory::createNewWithAttributes(outAttribs, getInputSchema(0));
		} else {
			return SDFSchemaFactory::createNewSchema("<tmp>", class(Tuple), outAttribs);		
		}		
	}
	
	
	po createOutput(List existingResults, Long groupID,	PointInTime timestamp, int inPort,	Map groupsToProcess, IGroupProcessor g) {

		PointInTime border = timestamp;

		if (existingResults != null) {
			produceResults(existingResults, groupID, g);
		}


		for (Object obj : groupsToProcess.entrySet()) {
			Entry entry = obj;
			AggregateTISweepArea sa = entry.getValue();

			Iterator results = sa.extractElementsBefore(timestamp);
				
			produceResults(results, entry.getKey(), g);
			PointInTime sa_min_ts = sa.calcMinTs();

			if (sa_min_ts != null) {
				if (sa_min_ts.before(border)) {
					border = sa_min_ts;
				}
			}
		}		
		transferArea.newHeartbeat(border, inPort);
	}
	
	po produceResults(List results, Long groupID, IGroupProcessor g) {
		for (Object obj : results) {
			PairMap e = obj;
			Tuple out = g.createOutputElement(groupID, e);
			ITimeInterval newMeta = e.getMetadata().clone();
			out.setMetadata(newMeta);
			transferArea.transfer(out);
		}
	}
	
	po  produceResults(Iterator results, Long groupID, IGroupProcessor g) {
		while (results.hasNext()) {
			PairMap e = results.next();
			Tuple out = null;
			PairMap r = calcEval(e, true);
			out = g.createOutputElement(groupID, r);
			ITimeInterval newMeta = e.getMetadata().clone();
			out.setMetadata(newMeta);
			transferArea.transfer(out);
		}
	}
	
	po calcEval(PairMap toEval, boolean clearPartialAggregate) : PairMap{
		PairMap ret = new PairMap();
		for (Object obj: toEval.entrySet()) {
			Entry e = obj;
			IEvaluator eva = eval.get(e.getKey());
			Tuple value = eva.evaluate(e.getValue());
			ret.put(e.getKey(), value);
			if (clearPartialAggregate){
				IPartialAggregate agg = e.getValue();
				agg.clear();
			}
		}
		return ret;
	}
	
	po updateSA(AggregateTISweepArea sa, Tuple elemToAdd, boolean outputPA) : List {
		List returnValues = new LinkedList();
		ITimeInterval t_probe = elemToAdd.getMetadata();
		Iterator qualifies = sa.extractOverlaps(t_probe);
		
		if (!qualifies.hasNext()) {
			saInsert(sa, calcInit(elemToAdd), t_probe);
		} else {
			List pl = getSortedIntersectionPoints(t_probe, qualifies);

			Iterator pointIter = pl.iterator();
			_Point p1 = null;
			_Point p2 = null;


			p1 = pointIter.next();
			PairMap lastPartialAggregate = p1.getLoad();
			
			while (pointIter.hasNext()) {
				p2 = pointIter.next();
				if (p1.before(p2)) {

					if (p1.isStart() && p2.isStart()) {
						lastPartialAggregate = updateSAStartStart(sa, elemToAdd, p1, p2, lastPartialAggregate);

					} else if (p1.isStart() && p2.isEnd()) {

						updateSAStartEnd(sa, elemToAdd, outputPA, returnValues, t_probe, p1, p2, lastPartialAggregate);

					} else if (p1.isEnd() && p2.isStart()) {
						updateSAEndStart(sa, elemToAdd, p1, p2);
					} else if (p1.isEnd() && p2.isEnd()) { // Element after
						lastPartialAggregate = updateSAEndEnd(sa, elemToAdd, p1, p2, lastPartialAggregate);
					}
					if (p1.getLoad != null) {
						lastPartialAggregate = p1.getLoad();						
					} 
				} else {
					if (p2.getLoad != null) {
						lastPartialAggregate = p2.getLoad();						
					} 
				}

				p1 = p2;
			}
		}
		return returnValues;
	}	
	
	po updateSAStartEnd(AggregateTISweepArea sa, Tuple elemToAdd, boolean outputPA, List returnValues, ITimeInterval t_probe,_Point p1,_Point p2, PairMap lastPartialAggregate) {
		boolean createNew = false;
		if (!outputPA && p2.before(t_probe.getStart())) {
			createNew = false;
			PairMap v = calcEval(lastPartialAggregate, false);
			ITimeInterval meta = lastPartialAggregate.getMetadata().clone();
			v.setMetadata(meta);
			meta.setEnd(p2.getPoint());
			returnValues.add(v);
		} else {
			ITimeInterval meta = lastPartialAggregate.getMetadata();
			createNew = !(p1.getPoint().equals(meta.getStart())
					&& p2.getPoint().equals(meta.getEnd()));
		}

		ITimeInterval newMeta = metadataMerge.mergeMetadata(lastPartialAggregate.getMetadata(), elemToAdd.getMetadata());
		newMeta = newMeta.clone();
		newMeta.setStartAndEnd(p1.getPoint(), p2.getPoint());
		saInsert(sa, calcMerge(lastPartialAggregate, elemToAdd, createNew), newMeta);
	}
	
	po calcMerge(PairMap toMerge, Tuple element, boolean createNew) : PairMap {		
		PairMap ret = new PairMap();
		for (Object obj : toMerge.entrySet()) {
			Entry e = obj;
			IMerger mf = merger.get(e.getKey());
			IPartialAggregate pa = mf.merge(e.getValue(), element, createNew);
			ret.put(e.getKey(), pa);
		}

		return ret;
	}

	
	po getSortedIntersectionPoints(ITimeInterval t_probe, Iterator qualifies) : List {		
		List pl = new LinkedList();

		while (qualifies.hasNext()) {
			PairMap element_agg = qualifies.next();
			ITimeInterval t_agg = element_agg.getMetadata();
			pl.add(new _Point(t_agg.getStart(), true,element_agg));
			pl.add(new _Point(t_agg.getEnd(), false, element_agg));
		}
		pl.add(new _Point(t_probe.getStart(), true,	null));
		pl.add(new _Point(t_probe.getEnd(), false,	null));

		Collections::sort(pl);
		return pl;
	}

	po updateSAStartStart(AggregateTISweepArea sa, Tuple elemToAdd,	_Point p1, _Point p2, PairMap lastPartialAggr) : PairMap{
		PairMap lastPartialAggregate = lastPartialAggr;
		if (p1.getLoad == null) {			
			ITimeInterval meta = elemToAdd.getMetadata();
			ITimeInterval newMeta = meta.clone();
			newMeta.setStartAndEnd(p1.getPoint(), p2.getPoint());
			saInsert(sa, calcInit(elemToAdd), newMeta);
			lastPartialAggregate = p2.getLoad();
		} else {
			PairMap pairMap = p1.getLoad();	
			ITimeInterval meta = pairMap.getMetadata();
			ITimeInterval newMeta = meta.clone();
			newMeta.setStartAndEnd(p1.getPoint(), p2.getPoint());
			saInsert(sa, pairMap, newMeta);
		}
		return lastPartialAggregate;
	}
	
	po updateSAEndEnd(AggregateTISweepArea sa, Tuple elemToAdd,	_Point p1,	_Point p2,	PairMap partialAggregate) : PairMap {
		if (p2.getLoad == null) {
			updateSAEndStart(sa, elemToAdd, p1, p2);
		} else {
			PairMap newPA = partialAggregate.clone();
			ITimeInterval newTI = partialAggregate.getMetadata().clone();
			newTI.setStartAndEnd(p1.getPoint(), p2.getPoint());
			saInsert(sa, newPA, newTI);
			partialAggregate = newPA;
		}
		return partialAggregate;
	}
	
	po updateSAEndStart(AggregateTISweepArea sa, Tuple elemToAdd,_Point p1,_Point p2) {
		ITimeInterval newTI = elemToAdd.getMetadata().clone();
		newTI.setStartAndEnd(p1.getPoint(), p2.getPoint());
		saInsert(sa, calcInit(elemToAdd), newTI);
	}
	
	
	po calcInit(Tuple element) : PairMap {
		PairMap ret = new PairMap();
		for (Object o : aggregationsMap.keySet()) {
			SDFSchema attrList = o;
			if (SDFSchema::subset(attrList, getInputSchema(0))) {
				Map funcs = aggregationsMap.get(attrList);
				for (Object obj : funcs.entrySet()) {		
					Entry e = obj;			
					FESortedClonablePair toFind = new FESortedClonablePair(attrList, e.getKey());
					IInitializer initFktn = init.get(toFind);					
					IPartialAggregate pa = initFktn.init(element);
					ret.put(attrList, e.getKey(), pa);
				}
			}
		}
		return ret;
	}
	
	po saInsert(AggregateTISweepArea sa, PairMap elem, ITimeInterval t) {
		elem.setMetadata(t);
		sa.insert(elem);
	}	
	
	ao addGroupingAttributes(SDFSchema attributes) {
		for (SDFAttribute a : attributes) {
			addGroupingAttribute(a);
		}
	}
	
	ao addGroupingAttribute(SDFAttribute attribute) {
		if (groupingAttributes.contains(attribute)) {
			return;
		}
		groupingAttributes.add(attribute);
		outputAttributList.add(new Pair(attribute, false));
		setOutputSchema(null);
	}
	
	ao addAggregation(SDFAttribute[] attributes, AggregateFunction function, SDFAttribute outAttribute) {
		SDFSchema schema = SDFSchemaFactory::createNewSchema("", class(Tuple), attributes);
		addAggregation(schema, function, outAttribute);
	}

	ao addAggregation(SDFSchema attributes,	AggregateFunction function, SDFAttribute outAttribute) {	
		outputAttributList.add(new Pair(outAttribute, true));
		setOutputSchema(null);
		Map af = aggregationsMap.get(attributes);
		if (af == null) {
			af = new HashMap();
			aggregationsMap.put(attributes, af);
		}
		af.put(function, outAttribute);
		outAttributeToAggregation.put(outAttribute, function);
	}
}


class _Point implements Comparable {
	PointInTime point;

	boolean isStartPoint;

	Object load;

	_Point(PointInTime p, boolean isStartPoint, Object element_agg) {
		this.point = p;
		this.isStartPoint = isStartPoint;
		this.load = element_agg;
	}


	override compareTo(Object obj) : int {
		_Point p2 = obj;
		int c = this.point.compareTo(p2.point);
		if (c == 0) {
			if (this.isStartPoint && !p2.isStartPoint) {
				c = 1;
			} else if (!this.isStartPoint && p2.isStartPoint) {
				c = -1;
			}
		}
		if (c == 0) {
			if (this.newElement()) {
				if (this.isStartPoint) {
					c = 1;
				} else {
					c = -1;
				}
			}
		}
		return c;
	}

	isStart() : boolean {
		return isStartPoint;
	}

	isEnd() : boolean {
		return !isStartPoint;
	}

	getLoad() : Object {
		return load;
	}

	getPoint() : PointInTime{
		return point;
	}

	before(_Point other) : boolean{
		return point.before(other.point);
	}

	before(PointInTime other) : boolean{
		return point.before(other);
	}

	override hashCode() : int {
		int PRIME = 31;
		int result = 1;
		if (point == null) {
			result = PRIME * result + 0;			
		} else {
			result = PRIME * result + point.hashCode();			
		}
		if (isStartPoint) {
			result = PRIME * result + 1231;			
		} else {
			result = PRIME * result + 1237;			
		}
		return result;
	}

	override equals(Object obj) : boolean {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (getClass() != obj.getClass())
			return false;
		_Point other =  obj;
		if (point == null) {
			if (other.point != null)
				return false;
		} else if (!point.equals(other.point))
			return false;
		if (isStartPoint != other.isStartPoint)
			return false;
		return true;
	}

	newElement() : boolean {
		return load == null;
	}

	override toString() : String {
		String result = "";
		if (isStartPoint) {
			result = "s";
		} else {
			result =  "e";
		}
		if (newElement()) {
			result = result +"^";
		} 
		return result + point;
	}
}


